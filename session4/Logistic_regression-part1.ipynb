{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression homework based on Andrew Ng's course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt, where, zeros, e, array, log, ones, append, linspace\n",
    "from pylab import scatter, show, legend, xlabel, ylabel, contour, title\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "Let's start by implementing the Sigmoid funcion. Remeber that this function should be able to operate every element inside a matrix.\n",
    "\n",
    "Here is the formula:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return # Write here your implementation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cost function\n",
    "\n",
    "Now you will implement the cost function and the gradient for logistic regression. Complete the function to return the cost and gradient. We will use this in the scipy minimize function later on.\n",
    "\n",
    "First recall that the logistic regression hypothesis is defined as:\n",
    "$$h_\\theta(x) = g(\\theta^Tx)$$\n",
    "\n",
    "The cost function for the logistic regression is defined as:\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum^m_{i=1}[-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1 - y^{(i)})\\log(1-h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "And the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$ element is defined as:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta,X,y):\n",
    "    theta = theta.reshape(-1,1)\n",
    "    m = len(y)\n",
    "    # Write here your implementation...\n",
    "    return float(J), grad.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict function\n",
    "\n",
    "We also need a function to evaluate the parameters learned. Here we use a dataset $X$ and a learned parameter vector $\\theta$ in order to produce \"1\" or \"0\". Try to implement this part using a list comprehension. You should try using a conditional statement along a for loop inside \"[ ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta,X):\n",
    "    hx = # Calculate the hypothesis\n",
    "    return # Return a vector with 0 if the value of h(x) is less than 0.5 and 1 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load our data, graph it and learn a vector of parameters $\\theta$ using the Scipy minimize function. We will leave this part of the code implemented so you can have a guide for the next implementation.\n",
    "\n",
    "Note:\n",
    "The visualization code was obtained from http://aimotion.blogspot.com.co/2011/11/machine-learning-with-python-logistic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('ex2data1.txt', delimiter=',')\n",
    "\n",
    "m1 = len(data1)\n",
    "\n",
    "X1 = data1[:,[0,1]]\n",
    "y1 = data1[:,2].reshape(m1,1)\n",
    "\n",
    "pos = where(y1 == 1)\n",
    "neg = where(y1 == 0)\n",
    "scatter(X1[pos, 0], X1[pos, 1], marker='o', c='b')\n",
    "scatter(X1[neg, 0], X1[neg, 1], marker='x', c='r')\n",
    "xlabel('Exam 1 score')\n",
    "ylabel('Exam 2 score')\n",
    "legend(['Admitted', 'Not Admitted'])\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.insert(X1,0,1,axis=1)\n",
    "\n",
    "m1,n1 = X1.shape\n",
    "\n",
    "lamb = 1\n",
    "\n",
    "initial_theta1 = np.zeros((n1,1))\n",
    "\n",
    "cost1, grad1 = costFunction(initial_theta1,X1,y1)\n",
    "\n",
    "print(\"Cost function with a parameter vector initialized with zeros: (0.693)\\n\" + str(cost1))\n",
    "print(\"Gradient vector of the cost function: ([-0.1        -12.00921659 -11.26284221])\\n\" + str(grad1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = minimize(costFunction, initial_theta1, args=(X1, y1), method=\"BFGS\", jac=True, options={'maxiter':400})\n",
    "print(\"OptimizeResult object obtained from the minimize function from Scipy:\\n\"\n",
    "      \"(You should only look at the resulting vector x, which should be this:[-25.16133284,   0.2062317 ,   0.2014716 ])\\n\\n\"\n",
    "      + str(theta1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
