{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression homework based on Andrew Ng's course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt, where, zeros, e, array, log, ones, append, linspace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "Let's start by implementing the Sigmoid funcion. Remeber that this function should be able to operate every element inside a matrix.\n",
    "\n",
    "Here is the formula:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return # Write here your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Cost Function\n",
    "\n",
    "We have now implemented most necesary functions for logistic regression. Now, we are going to implement the regularized version of logistic regression.\n",
    "\n",
    "In order to achieve this you need to modify your implementation of the cost function in order to include regularization. Please do this in the next function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta,X,y,lamb):\n",
    "    theta = theta.reshape(-1,1)\n",
    "    m = len(y)\n",
    "    # Write here your implementation\n",
    "    return float(J),grad.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding synthetic features\n",
    "\n",
    "In order to better fit more complex figures in linear regression, we can add synthetic features multiplying two or more features. Remember to be careful if you use this, because this can lead you more easily to overfit your data.\n",
    "\n",
    "We provide a similar implementation of the feature mapping function used in the Andrew Ng's course in order to compute a similar result from the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1,X2,degree=6):\n",
    "    out = np.ones(len(X1)).reshape(len(X1),-1)\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            val = np.power(X1,i-j) * np.power(X2,j)\n",
    "            out = np.append(out,val,axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to the previous part, we load our data, graph it and learn a vector of parameters $\\theta$ using the Scipy minimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('ex2data2.txt', delimiter=',')\n",
    "\n",
    "m2 = len(data2)\n",
    "\n",
    "realX = X2 = data2[:,[0,1]]\n",
    "realY = y2 = data2[:,2].reshape(m2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = where(y2 == 1)\n",
    "neg = where(y2 == 0)\n",
    "plt.scatter(X2[pos, 0], X2[pos, 1], marker='o', c='b')\n",
    "plt.scatter(X2[neg, 0], X2[neg, 1], marker='x', c='r')\n",
    "plt.xlabel('Microchip Test 1')\n",
    "plt.ylabel('Microchip Test 2')\n",
    "plt.legend(['Accepted', 'Rejected'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the data into the model, first we generate new features using the *mapFeature* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = mapFeature(X2[:,0].reshape(m2,-1),X2[:,1].reshape(m2,-1))\n",
    "\n",
    "# Get new lenghts\n",
    "m2,n2 = X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assing a value to the parameter lambda\n",
    "lamb = 1\n",
    "\n",
    "initial_theta2 = np.zeros((n2,1))\n",
    "\n",
    "cost2, grad2 = costFunctionReg(initial_theta2,X2,y2,lamb)\n",
    "\n",
    "print(\"Cost function with initial paremeters theta = 0: (0.693)\\n\" + str(cost2))\n",
    "print(\"\"\"Gradient vector:\n",
    "(The answer should be similar to this one:\n",
    "[8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02\n",
    " 1.15013308e-02 3.76648474e-02 1.83559872e-02 7.32393391e-03\n",
    " 8.19244468e-03 2.34764889e-02 3.93486234e-02 2.23923907e-03\n",
    " 1.28600503e-02 3.09593720e-03 3.93028171e-02 1.99707467e-02\n",
    " 4.32983232e-03 3.38643902e-03 5.83822078e-03 4.47629067e-03\n",
    " 3.10079849e-02 3.10312442e-02 1.09740238e-03 6.31570797e-03\n",
    " 4.08503006e-04 7.26504316e-03 1.37646175e-03 3.87936363e-02])\n",
    "\n",
    "\"\"\" + str(grad2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the Scipy minimize function to train our model with the BFGS optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta2 = minimize(costFunctionReg, initial_theta2, args=(X2, y2, lamb), method=\"BFGS\", jac=True, options={'maxiter':400})\n",
    "print(\"\"\"OptimizeResult object obtained from the minimize function from Scipy:\n",
    "(You should only look at the resulting array x, which should be similar to this:\n",
    "[ 1.27268739,  0.62557016,  1.1809665 , -2.01919822, -0.91761468,\n",
    "-1.43194199,  0.12375921, -0.36513086, -0.35703388, -0.17485805,\n",
    "-1.45843772, -0.05129676, -0.61603963, -0.2746414 , -1.19282569,\n",
    "-0.24270336, -0.20570022, -0.04499768, -0.27782709, -0.29525851,\n",
    "-0.45613294, -1.04377851,  0.02762813, -0.29265642,  0.01543393,\n",
    "-0.32759318, -0.14389199, -0.92460119])\n",
    "\n",
    "\"\"\" + str(theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a bonus, we now draw the decision boundary obtained from the trained model.\n",
    "\n",
    "Feel free to play with different values for $\\lambda$ to see how it overfits the data with small regularization values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = np.linspace(-1, 1.5, 50)\n",
    "yplot = np.linspace(-1, 1.5, 50)\n",
    "xplot, yplot = np.meshgrid(xplot, yplot)\n",
    "zplot = np.array([[(mapFeature([[xplot[x,y]]], [[yplot[x,y]]])@theta2.x)[0]\n",
    "                   for x in range(50)]\n",
    "                  for y in range(50)])\n",
    "pos = where(realY == 1)\n",
    "neg = where(realY == 0)\n",
    "\n",
    "ac = plt.scatter(realX[pos, 0], realX[pos, 1], marker='o', c='b')\n",
    "rx = plt.scatter(realX[neg, 0], realX[neg, 1], marker='x', c='r')\n",
    "cplot = plt.contour(xplot, yplot, zplot, 0)\n",
    "cplot.collections[0].set_label(\"\")\n",
    "#plt.clabel(cplot, fmt={0: \"Decision boundary\"})\n",
    "plt.xlabel('Microchip Test 1')\n",
    "plt.ylabel('Microchip Test 2')\n",
    "plt.legend(['Accepted', 'Rejected', \"Decision boundary\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
